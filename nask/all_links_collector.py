import requests
from bs4 import BeautifulSoup
import functools
import time
from playwright.sync_api import sync_playwright


# ---LOGGER SETUP ------------------------------------------------------------
import logging

logging.basicConfig(
    level=logging.DEBUG,  # Set the logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    format='%(asctime)s - %(levelname)s - %(message)s',  # Log message format
    handlers=[
        logging.FileHandler("app.log", encoding='utf-8'),  # Write logs to a file
        logging.StreamHandler()  # Print logs to the console
    ]
)

logger = logging.getLogger(__name__)
# -----------------------------------------------------------------------------


# ---RETRY DECORATOR ----------------------------------------------------------
def retry(exceptions=(Exception,), max_attempts=2, delay=1):
    """
    Retry decorator that retries the decorated function only when specific exceptions occur.
    
    Args:
        exceptions: Tuple of exception classes that should trigger retry
        max_attempts: Maximum number of retry attempts
        delay: Delay between retries in seconds
    """
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            attempts = 0
            while attempts <= max_attempts:
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    attempts += 1
                    if attempts > max_attempts:
                        logger.error(f"Failed after {max_attempts} attempts: {e}")
                        raise
                    logger.warning(f"Attempt {attempts} failed with {e.__class__.__name__}: {e}. Retrying in {delay} seconds...")
                    time.sleep(delay)
                except Exception as e:
                    # For any other exceptions, don't retry
                    logger.error(f"Failed with non-retryable exception: {e}")
                    raise
        return wrapper
    return decorator


# custom exception
class AbsentAnchorElementException(Exception):
    pass



# MAIN FUNCTION 
@retry(exceptions=(AbsentAnchorElementException,), max_attempts=2, delay=1)
def get_all_links_of_articles_until_lastsaved_met():
    # here using simple requests is sufficient 
    url = "https://nask.pl/aktualnosci"
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        context = browser.new_context()
        page = context.new_page()
        try:
            page.goto(url)
            page.wait_for_load_state("networkidle")
        except Exception as e:
            logger.critical(f"Failed to load page {url}: {e}")
            raise AbsentAnchorElementException(f"Failed to load page {url}: {e}") from e

        html_content = page.content()
        browser.close()

    logger.debug("Page loaded successfully, proceeding to parse HTML content.")

    soup = BeautifulSoup(html_content, "html.parser")

    logger.debug("HTML content parsed successfully.")

    unfiltered_article_links = []

    articles_container = soup.select_one('body > main > section > div:nth-child(1) > div > div.flex.w-full.flex-col.gap-size-09')    # XPath is not the best choice, but I can't see an option to locate it by other means for now, so I'll leave it as it is for now

    logger.debug(f"Articles container found: {articles_container is not None}")

    articles = articles_container.select('div.flex.flex-col')

    logger.debug(f"Number of articles found: {len(articles)}")
    
    for article in articles:
        link_element = article.select_one('a')
        logger.debug(f"Link element found: {link_element is not None}")
        if link_element:
            link = link_element.get('href')
            unfiltered_article_links.append(link)
        else:
            logger.warning(f"No link found in article")


    article_links = []
    # filtering out article links we need
    for article_link in unfiltered_article_links:
        if article_link.strip().startswith('/aktualnosci/'):
            full_link = "https://nask.pl" + article_link.strip()
            if full_link not in article_links:
                article_links.append(full_link)


    
    # getting the last saved link met | pobieranie ostatniego zapisanego linku
    with open("nask/lastsaved_articlelink.txt", "r") as file:
        lastsaved_articlelink = file.read()
    
    collected_links_list = []
    
    for link in article_links:
        if link == lastsaved_articlelink:
            break
        
        collected_links_list.append(link)

    if collected_links_list:
        lastsaved_articlelink = collected_links_list[0]

    with open("sekurak/lastsaved_articlelink.txt", "w") as file:
        file.write(lastsaved_articlelink)

    for link in collected_links_list:
        print(link)
        time.sleep(0.5)

    return collected_links_list






if __name__ == "__main__":
    get_all_links_of_articles_until_lastsaved_met()


